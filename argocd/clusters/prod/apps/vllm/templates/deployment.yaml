apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-mistral-7b
  namespace: vllm-ai
  labels:
    app: vllm-mistral-7b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-mistral-7b
  template:
    metadata:
      labels:
        app: vllm-mistral-7b
    spec:
      volumes:
      - name: cache-volume
        persistentVolumeClaim:
          claimName: vllm-mistral-7b-pvc
      # vLLM needs to access the host's shared memory for tensor parallel inference.
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: "2Gi"
      nodeSelector:
        gpu: "true"
      tolerations:
        - key: "gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"    
      containers:
      - name: vllm-mistral-7b
        image: vllm/vllm-openai:latest
        command: ["vllm"]
        args:
        - "serve"
        - "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
        - "--trust-remote-code"
        - "--enable-chunked-prefill"
        - "--max-num-batched-tokens"
        - "1024"
        # VRAM controls
        - "--tensor-parallel-size"
        - "1"
        - "--gpu-memory-utilization"
        - "0.0"         # start conservative
        - "--max-model-len"
        - "1024"         # 1024 is safer on a 10GB card
        - "--swap-space"
        - "8"            # GB of host RAM
        # switch to a quantized checkpoint:
        - "--quantization"
        - "awq"
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: vllm-hf-token
              key: HUGGING_FACE_HUB_TOKEN
        - name: VLLM_LOGGING_LEVEL
          value: DEBUG
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "expandable_segments:True"  
        - name: LD_LIBRARY_PATH
          value: "/usr/local/nvidia/lib:/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}"  
        ports:
        - containerPort: 8000
        resources:
          limits:
            cpu: "10"
            memory: 20G
            nvidia.com/gpu: "1"
          requests:
            cpu: "2"
            memory: 6G
            nvidia.com/gpu: "1"
        volumeMounts:
        - mountPath: /root/.cache/huggingface
          name: cache-volume
        - name: shm
          mountPath: /dev/shm
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 5